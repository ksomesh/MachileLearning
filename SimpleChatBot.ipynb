{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcPPJoBmH9yXVDy2nFW22q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksomesh/MachileLearning/blob/master/SimpleChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=c_gXrw1RoKo\n",
        "\n",
        "Architecture\n",
        "\n",
        "Chat Window  <--> Interface <--> NLP Model <--> corpus and App DB\n",
        "\n",
        "Create ChatBot Steps\n",
        "1. Import corpus\n",
        "2. PreProcess the data\n",
        "3. Text case handling\n",
        "4. Tokenization\n",
        "5. Stemming\n",
        "6. Bag of words\n",
        "7. One hot encoding"
      ],
      "metadata": {
        "id": "gTP_dbyNDV77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKnhgjBehF67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and read corppus"
      ],
      "metadata": {
        "id": "qAdxb0ZyEZ6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prog from https://www.youtube.com/watch?v=c_gXrw1RoKo\n",
        "f = open(\"chatbot.txt\", \"r\", errors=\"ignore\") #corpus , this file just have text from data science wiki\n",
        "rawDoc = f.read()\n",
        "rawDoc=rawDoc.lower()\n",
        "nltk.download(\"punkt\") # use punkt tokenizer\n",
        "nltk.download(\"wordnet\") # use wordnet dictionary\n",
        "sentTokens = nltk.sent_tokenize(rawDoc) # Convert doc to list of sentences\n",
        "wordTokens = nltk.word_tokenize(rawDoc) # Convert doc to list of words\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lLcUqF8hldz",
        "outputId": "bfc03a08-8bc4-4edf-aad9-facafe946b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of sentence and word token"
      ],
      "metadata": {
        "id": "NCCWKPrCEiVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentTokens[:2]\n",
        "wordTokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6W3LHnrlnEs",
        "outputId": "ca5ce618-d379-4175-9fba-e6ee907f35d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from', 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "7EUXlb8eG3uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer() #\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "removePunctDict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(removePunctDict)))"
      ],
      "metadata": {
        "id": "hbjKqiZymWAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greeting Function\n"
      ],
      "metadata": {
        "id": "BLn5Ws7PznV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listGreetInputs = (\"hello\", \"hi\", \"watsup\", \"greetings\",  \"sup\", \"hey\")\n",
        "listGreetResp = (\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\")\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in  sentence.split():\n",
        "    if word.lower() in listGreetInputs:\n",
        "      return random.choice(listGreetResp)"
      ],
      "metadata": {
        "id": "8DbCMpXezsZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation"
      ],
      "metadata": {
        "id": "Gkim5l0o1KyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "-A8PUDDJ1OoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(userResponse):\n",
        "  robo1Resp = ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  Tfidf = TfidfVec.fit_transform(sentTokens)\n",
        "  vals = cosine_similarity(Tfidf[-1], Tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  reqTfidf = flat[-2]\n",
        "  if (reqTfidf == 0):\n",
        "    robo1Resp += \"I am sorry, I don't understand you\"\n",
        "    return robo1Resp\n",
        "  else: \n",
        "    robo1Resp += sentTokens[idx]\n",
        "    return robo1Resp\n"
      ],
      "metadata": {
        "id": "KwFmHA2C1zLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define start and end protocol"
      ],
      "metadata": {
        "id": "uh52xlQZ6EXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"Bot: My name is Shamik. Lets have an conversation! Also, if you want to exit anytime, just type Bye!\")\n",
        "while(flag== True):\n",
        "  userResp = input()\n",
        "  userResp = userResp.lower()\n",
        "  if (userResp!= 'bye'):\n",
        "    if (userResp == 'thanks' or userResp == 'thankyou'):\n",
        "      flag = False\n",
        "      print(\"BOT: you are welcome!\")\n",
        "    else :\n",
        "      if(greet(userResp) != None):\n",
        "        print(\"BOT:\" + greet(userResp))\n",
        "      else:\n",
        "        sentTokens.append(userResp)\n",
        "        wordTokens = wordTokens + nltk.word_tokenize(userResp)\n",
        "        finalWords = list(set(wordTokens))\n",
        "        print(\"BOT: \" , end=\"\")\n",
        "        print(response(userResp))\n",
        "        sentTokens.remove(userResp)\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"BOT: GoodBye, Take care <3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncpuVB9l6V4Q",
        "outputId": "c464a8c1-9cdf-430e-bb46-d68bfa0c86c6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: My name is Shamik. Lets have an conversation! Also, if you want to exit anytime, just type Bye!\n",
            "Bye\n",
            "BOT: GoodBye, Take care <3\n"
          ]
        }
      ]
    }
  ]
}